# Классификатор спам-сообщений (SMS Spam)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BRSHD/spam-classifier/blob/main/spam_classifier.ipynb)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Модель машинного обучения для бинарной классификации SMS-сообщений на спам (spam) и не-спам (ham). Проект включает в себя полный цикл: от исследования данных (EDA) до обучения модели и оценки её эффективности.

## Содержание
- [Запуск](#запуск)
- [Данные](#данные)
- [Технологии и библиотеки](#технологии)
- [Основные этапы проекта](#основные-этапы-проекта)
- [Результаты](#результаты)
- [Лицензия](#лицензия)

## Запуск

### В Google Colab (Рекомендуется)

Самый простой способ открыть и запустить проект — использовать Google Colab:

1.  **Откройте ноутбук в Colab:** Нажмите на кнопку **"Open in Colab"** вверху этого файла.
2.  **Подключите среду выполнения:** В Colab нажмите `Runtime` → `Run all` для выполнения всех ячеек ноутбука.

### Локальный запуск

Если вы хотите запустить проект на своем компьютере, выполните следующие шаги:

#### 1. Клонируйте репозиторий
```bash
git clone https://github.com/BRSHD/spam-classifier.git
cd spam-classifier
```

#### 2. Создайте и активируйте виртуальное окружение
```bash
python -m venv venv

venv\Scripts\activate # Windows

source venv/bin/activate # Linux/macOS
```

#### 3. Установите зависимости

```bash
pip install -r requirements.txt
```

#### 4. Запустите Jupyter Notebook

```bash
jupyter notebook spam_classifier.ipynb
```

## Данные

Используется открытый датасет [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) с Kaggle.

**Размер:** 5572 сообщения

**Классы**:

-***Ham (Не спам):*** 4825 сообщений (86.6%)

-***Spam (Спам):*** 747 сообщений (13.4%)

**Признаки**: label (метка: spam/ham), text (текст сообщения).

## Технологии

**Язык:** Python 3.12+

**Анализ данных:** Pandas, NumPy

**Визуализация:** Matplotlib, Seaborn

**Машинное обучение:** Scikit-learn

**Обработка текста:** NLTK, re (регулярные выражения)

**Среда:** Google Colab

## Основные этапы проекта

✅Загрузка и исследование данных (EDA):

- Анализ распределения классов (выявлен сильный дисбаланс).

- Визуализация данных (круговые диаграммы, гистограммы, boxplot).

- Анализ длины сообщений в разрезе классов.

✅Предобработка текста:

- Приведение к нижнему регистру.

- Удаление пунктуации и стоп-слов.

- Лемматизация.

✅Векторизация текста:

- Преобразование текстов в числовые векторы с помощью TfidfVectorizer.

✅Обучение моделей: Naive Bayes (MultinomialNB), Logistic Regression.

✅Оценка моделей(accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC).

## Результаты

Модель логистической регрессии показала наилучшие результаты.

|                  | Accuracy | Precision | Recall  | F1-Score | ROC-AUC | PR-AUC  |
| :---------------------- | :------- | :-------- | :------ | :------- | :------ | :------ |
| Logistic Regression | 0.9794 | 0.9922  | 0.8523 | 0.9170 | 0.9975 | 0.9861 |
| Naive Bayes             | 0.9722   | 0.9917      | 0.7987   | 0.8848    | 0.9858   | 0.9613   |

### Ключевые причины превосходства Logistic Regression:

1. *Учет взаимосвязей между признаками*
   
-**Naive Bayes:** предполагает полную независимость признаков (слов в тексте), что редко выполняется в реальности
   
-**Logistic Regression:** учитывает корреляции между словами, что более реалистично для естественного языка

2. *Лучшая работа с дисбалансом классов*
   
-В нашем датасете: 86.6% ham vs 13.4% spam

-**Logistic Regression:** лучше справляется с дисбалансом, что видно по более высокому Recall (0.8523 vs 0.7987)

Высокий Precision (0.99) означает, что почти все сообщения, которые модель пометила как спам, и правда им являются. Recall (0.85) показывает, что модель находит 85% всего спама из датасета.

## TO DO

- Эксперименты с другими методами векторизации (Word2Vec, BERT).
- Более сложные модели (CatBoost, LightGBM, может быть нейросети).
- Углубленная работа с дисбалансом классов (SMOTE).
- Деплой модели как микросервиса.

## Лицензия
Этот проект распространяется под лицензией MIT. Подробнее см. в файле LICENSE.
